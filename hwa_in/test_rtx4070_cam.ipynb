{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fdc808",
   "metadata": {},
   "source": [
    "# RTX 4070 GPU 환경에서 우울증 판별 모델 개발\n",
    "\n",
    "이 노트북은 **NVIDIA GeForce RTX 4070**과 같이 VRAM이 약 8GB인 환경을 고려하여, 우울증 여부를 판별하는 이진 분류 모델을 학습하는 과정을 제공합니다. 앞서 설명한 데이터 구조와 감정 재분류를 그대로 사용하되, **작은 메모리 환경**에서도 학습이 가능하도록 배치 크기를 줄이고 혼합정밀도 학습(mixed precision training)을 적용합니다.\n",
    "\n",
    "데이터는 다음과 같은 구조를 가정합니다:\n",
    "```\n",
    "data/\n",
    "  train/\n",
    "    train_image/\n",
    "    train_label/\n",
    "  vali/\n",
    "    vali_image/\n",
    "    vali_label/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16a474",
   "metadata": {},
   "source": [
    "## 1. 패키지 불러오기와 기본 설정\n",
    "\n",
    "딥러닝 학습에 필요한 라이브러리들을 불러옵니다. RTX 4070 환경에서는 메모리가 한정적이므로 **혼합정밀도 학습(mixed precision training)**을 사용하여 메모리 사용량을 줄입니다. 이를 위해 `torch.cuda.amp` 모듈을 가져옵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# 혼합정밀도 학습을 위한 모듈\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 현재 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9f44a",
   "metadata": {},
   "source": [
    "## 2. 감정 레이블 정의\n",
    "\n",
    "감정 레이블을 한국어로 매핑하고 우울/비우울 두 클래스로 재분류합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eaf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = {\n",
    "    'anger': '분노',\n",
    "    'anxiety': '불안',\n",
    "    'hurt': '상처',\n",
    "    'joy': '기쁨',\n",
    "    'neutral': '중립',\n",
    "    'sadness': '슬픔',\n",
    "    'surprise': '당황'\n",
    "}\n",
    "DEPRESSION_EMOTIONS = ['anxiety', 'hurt', 'sadness']\n",
    "NON_DEPRESSION_EMOTIONS = ['anger', 'joy', 'neutral', 'surprise']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a362295",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 클래스와 전처리\n",
    "\n",
    "앞선 노트북과 동일하게 JSON 레이블을 파싱하여 (이미지 경로, 라벨) 리스트를 만들고, `EmotionDataset` 클래스로 데이터를 로드합니다. 혼합정밀도 학습은 데이터셋 정의와는 무관하므로 여기서는 동일한 구현을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def parse_label_files(label_dir, image_dir):\n",
    "    \"\"\"\n",
    "    JSON 라벨 파일을 읽어 이미지 경로와 이진 라벨(우울=1, 비우울=0)을 반환합니다.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    json_files = sorted(glob(os.path.join(label_dir, '*.json')))\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        for file_name, emotion in data.items():\n",
    "            img_path = os.path.join(image_dir, emotion, file_name)\n",
    "            label = 1 if emotion in DEPRESSION_EMOTIONS else 0\n",
    "            samples.append((img_path, label))\n",
    "    return samples\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    이미지 경로와 라벨을 받아 PyTorch Tensor로 변환하여 반환하는 데이터셋 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca783e",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리(변환) 정의\n",
    "\n",
    "학습 데이터에는 데이터 증강을 포함하고, 검증 데이터에는 기본 전처리만 적용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08cd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "vali_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f627f",
   "metadata": {},
   "source": [
    "## 5. 데이터셋 로딩과 클래스 불균형 처리\n",
    "\n",
    "메모리가 제한된 환경에 맞춰 **배치 크기(batch_size)**를 16으로 줄입니다. 나머지 로직은 이전과 동일하게 클래스를 균형 있게 샘플링하기 위해 `WeightedRandomSampler`를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bcdac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "train_label_dir = os.path.join(base_dir, 'train', 'train_label')\n",
    "train_image_dir = os.path.join(base_dir, 'train', 'train_image')\n",
    "vali_label_dir = os.path.join(base_dir, 'vali', 'vali_label')\n",
    "vali_image_dir = os.path.join(base_dir, 'vali', 'vali_image')\n",
    "\n",
    "train_samples = parse_label_files(train_label_dir, train_image_dir)\n",
    "vali_samples = parse_label_files(vali_label_dir, vali_image_dir)\n",
    "\n",
    "train_dataset = EmotionDataset(train_samples, transform=train_transform)\n",
    "vali_dataset = EmotionDataset(vali_samples, transform=vali_transform)\n",
    "\n",
    "labels = [label for _, label in train_samples]\n",
    "class_sample_count = np.bincount(labels)\n",
    "class_weights = 1. / class_sample_count\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "batch_size = 16  # RTX 4070 8GB VRAM을 고려하여 배치 크기 감소\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "class_counts = { '비우울(0)': int(class_sample_count[0]), '우울(1)': int(class_sample_count[1]) }\n",
    "class_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299705c",
   "metadata": {},
   "source": [
    "## 6. 모델 정의와 손실 함수 설정\n",
    "\n",
    "RTX 4070 환경에서도 높은 성능을 위해 **ResNet50** 모델을 사용하되, 혼합정밀도 학습을 적용합니다. 클래스 불균형을 해결하기 위한 가중치도 동일하게 적용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37507065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 혼합정밀도 학습을 위한 스케일러\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703b56b",
   "metadata": {},
   "source": [
    "## 7. 혼합정밀도 학습과 검증 루프 정의\n",
    "\n",
    "`torch.cuda.amp.autocast`와 `GradScaler`를 사용하여 학습 시 부동소수점 정밀도를 자동으로 조절합니다. 이는 GPU 메모리 사용량을 줄이고 계산 속도를 높이는 데 도움을 줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97186aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 혼합정밀도 정방향 패스\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # 스케일링된 손실로 역전파\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects.double() / total\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects.double() / total\n",
    "    return epoch_loss, epoch_acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61053c46",
   "metadata": {},
   "source": [
    "## 8. 모델 학습 실행\n",
    "\n",
    "배치 크기를 줄이고 혼합정밀도 학습을 적용하여 여러 에폭 동안 모델을 학습합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff387941",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    vali_loss, vali_acc = evaluate(model, vali_loader, criterion, device)\n",
    "    print(f'Epoch {epoch}/{num_epochs}: ',\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%',\n",
    "          f'| Val Loss: {vali_loss:.4f}, Val Acc: {vali_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb8cb2",
   "metadata": {},
   "source": [
    "## 9. 혼동 행렬 시각화\n",
    "\n",
    "검증 세트에 대한 혼동 행렬을 시각화하여 각 클래스의 예측 성능을 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['비우울', '우울'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix (Validation Set)')\n",
    "    plt.show()\n",
    "\n",
    "# plot_confusion_matrix(model, vali_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d8cdb",
   "metadata": {},
   "source": [
    "## 10. Grad-CAM을 이용한 시각화\n",
    "\n",
    "모델이 이미지의 어느 부분에 주목하여 우울/비우울을 판단하는지 시각적으로 확인하기 위해 **Grad-CAM**(Gradient-weighted Class Activation Mapping)을 적용합니다. Grad-CAM은 특정 클래스에 대한 출력을 기준으로 마지막 합성곱 층의 특성맵(feature map)과 그라디언트를 결합하여 중요 영역을 강조한 히트맵을 생성합니다.\n",
    "\n",
    "아래 코드에서는 ResNet50의 마지막 합성곱 층인 `model.layer4`에 후크(hook)를 등록하여 forward 패스에서의 feature map과 backward 패스에서의 gradient를 저장한 뒤, 이를 이용해 히트맵을 생성하고 원본 이미지 위에 오버레이합니다. 검증 데이터의 첫 번째 샘플을 예시로 사용하지만, 원하는 다른 이미지에 적용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_gradcam(model, input_tensor, target_class):\n",
    "    model.eval()\n",
    "    feature_maps = []\n",
    "    gradients = []\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        feature_maps.append(output.detach())\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients.append(grad_out[0].detach())\n",
    "    \n",
    "    # 마지막 합성곱 층(layer4)에 후크 등록\n",
    "    handle_f = model.layer4.register_forward_hook(forward_hook)\n",
    "    handle_b = model.layer4.register_backward_hook(backward_hook)\n",
    "\n",
    "    # forward pass\n",
    "    output = model(input_tensor.unsqueeze(0))\n",
    "    score = output[0, target_class]\n",
    "    # backward pass\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "\n",
    "    # 후크 해제\n",
    "    handle_f.remove()\n",
    "    handle_b.remove()\n",
    "\n",
    "    # gradients와 feature_maps는 리스트로 저장되므로 첫 번째 요소 사용\n",
    "    grads = gradients[0][0]  # shape: [C, H, W]\n",
    "    fmap = feature_maps[0][0]  # shape: [C, H, W]\n",
    "\n",
    "    # 각 채널별로 gradient를 평균내어 가중치 계산\n",
    "    weights = grads.mean(dim=(1, 2))\n",
    "    cam = torch.zeros(fmap.shape[1:], dtype=fmap.dtype).to(fmap.device)\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * fmap[i]\n",
    "    \n",
    "    cam = torch.relu(cam)\n",
    "    cam = cam - cam.min()\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "    cam = cam.cpu().numpy()\n",
    "    # 입력 이미지 크기로 리사이즈\n",
    "    cam = np.array(Image.fromarray(cam).resize((input_tensor.size(2), input_tensor.size(1))))\n",
    "    return cam\n",
    "\n",
    "\n",
    "def show_gradcam_on_image(input_tensor, cam_mask, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    # 입력 이미지는 정규화되어 있으므로 원래 스케일로 되돌림\n",
    "    img = input_tensor.cpu().permute(1,2,0).numpy()\n",
    "    img = img * np.array(std)[None, None, :] + np.array(mean)[None, None, :]\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    heatmap = cm.jet(cam_mask)[..., :3]  # RGBA 중 RGB만\n",
    "    # 히트맵과 원본 이미지 합성\n",
    "    overlay = heatmap * 0.4 + img\n",
    "    overlay = overlay / overlay.max()\n",
    "    \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Grad-CAM Overlay')\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 예시: 검증 데이터 첫 번째 샘플에 Grad-CAM 적용\n",
    "test_iter = iter(vali_loader)\n",
    "example_inputs, example_labels = next(test_iter)\n",
    "example_input = example_inputs[0].to(device)\n",
    "example_label = example_labels[0].item()\n",
    "\n",
    "# Grad-CAM 생성\n",
    "cam_mask = generate_gradcam(model, example_input, example_label)\n",
    "# 시각화\n",
    "show_gradcam_on_image(example_input.cpu(), cam_mask)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
