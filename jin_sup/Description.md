# 데이터 출처
[감정 분류 데이터셋](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=259)   

전체 영상과 이미지 파일로 구성 되어있음   
100명의 연기자가 참여했으며  
각자 7개의 감정을 분류하였음


# 데이터 세트 구조 
폴더 구조는 0~9, 10~19 와 같이 나위어져 있음   
총 100개의 폴더가 있었음

0~9 폴더를 클릭할 경우 000, 001 ... 009 순으로 폴더 구성
000 폴더를 클릭할 경우 movie, picture 폴더 구성
movie 파일을 클릭할 경우 350개에 가까운 영상 파일이 있었음

폴더마다 파일이 살짝씩 빠진게 있었음   
예를들어 000 폴더에는 24, 30번 파일이 빠져있고,
0001 폴더는 1~5번 파일이 없는 등 불규칙 하였음  

라벨링은 아래와 같이 파일명에 적힌 숫자로 해당 감정 상태를 
텍스트로만 적혀있었음 

1~50 : 행복(Happiness)   
51~100 : 놀람(Surprise)   
101~150 : 무 감정(Neutral)   
151~200 : 공포(Fear)   
201~250 : 역겨움, 혐오(Disgust)   
251~300 : 화남(Anger)   
301~350 : 슬픔(Sadness)

# 영상 -> 음성파일 변환

### 변환 이유
1. 데이터 형식을 동영상으로 유지해서 학습에 이용하기에는 동영상 File 이 매우 무거움

2. 음성 파일 변환을 하면서 데이터 세트 폴더 구조 개선을 하였음
- 데이터 위치의 가독성을 위해 데이터 세트 패키지 구조 변경
- (1번연기자_001.mp4, 1번연기자_002.mp4 ... 100번 연기자_0350.mp4 순서로 데이터를 하나의 폴더안에 전체 평탄화)
3. 데이터 세트의 포함된 불필요한 데이터(연기자들 표정 이미지)를 제거해서 학습용 데이터의 혼란을 제거

       
# 음성 데이터 전처리

### 1) 16 khz 리샘플링

음성 데이터 학습 진행시 학습 데이터끼리 같은 khz로 맞춰야 학습시 __호환성 유지 가능__

사람의 대역폭은 보통 8khz 이하 범위 안에 있음 하지만 나이퀴스트 정리를 사용해서 __16khz__사용

__나이퀴스트 정리?__   
아날로그 -> 디지털 신호로 변환시 신호의 포함된 최고 주파수의 2배 이상으로 샘플링 해야 원래 신호를 완벽하게 복원할 수 있다는 정리 

### 2) 음성 앞, 뒤 구간 30 데시벨이하 제거
연기자의 발화 타이밍이 사람마다 다름,   
무음 구간 비교로 학습에 영향가기 때문에 앞뒤 음성 구간을 사람의 목소리로 인정되는 30 데시벨이하로 짜름   
(앞,뒤만 짜르고 음성의 진행 중간은 건들지 않습니다.)

### 3) 음성 -> 이미지 파일 변환 
목소리의 억양 및 분위기 파악도 가중치에 포함시키기 위해 __Mel-spectrogram__ 을 통해 이미지 변환

# AI 모델 선택

모델 베이스를 빨리 확정하고 마지막에 하이퍼 파라미터 및 이미지 전처리로 F1_Score/Precision/Recell 계산에 집중하기 위해 먼저 모델을 선택 하였음

모델 테스트를 위해 각각 파인튜닝으로 Linear 의 out을 맞췄고 학습 테스트를 위해 모델에 맞는 이미지 리사이즈 적용 

테스트 모델 
- RESNET18 
- CONVNEXT_TINY
- CONVNEXT_SMALL
- CONVNEXT_BASE
- CONVNEXT_LARGE
- VIT_B_16
- EFFICIENT_NET_B0

CONVNEXT_SMALL 모델 최종 선택 
- F1_Score: 0.6886
- Precision: 0.6970
- Recell: 0.6882


# 이미지 전처리

### 1) 데이터 셋 기준 평균, 표준편차로 정규화
-> 음성 스펙트로그램(Grayscale)은 분포가 표준화된 이미지와 다름   
-> 기존 ImageNet 평균/표준편차(0.485, 0.456, 0.406 …)를 쓰면 내 데이터랑 분포가 다를 수 있음.   
-> 그래서 직접 mean/std를 구해서 맞춰주는 게 더 안정적이었음

### 2) 사진 빈 영역 잘라내기 
-> 입력 스펙트로그램에 불필요한 테두리가 포함될 수 있다.
-> 일정한 margin(여백)을 잘라내어, 모델이 주요 음성 패턴에 집중할 수 있도록 한다. 

### 3) 가로(시간) 방향띠 마스킹
-> 스펙트로그램의 **가로 방향 일부(시간 구간)**을 가려서, 발화 중 특정 순간 정보가 사라진 상황을 흉내 낸다.   
-> 이는 발화 끊김·잡음 등 시간적 결손 상황에도 강인한 모델을 만들기 위한 SpecAugment 기법이다.

### 4) 세로(주파수) 방향띠 마스킹
-> 스펙트로그램의 **세로 방향 일부(주파수 대역)**을 가려서, 특정 음역대가 손실된 상황을 흉내 낸다.   
-> 이는 마이크 특성, 잡음, 특정 주파수 손실 등에 대비해 모델이 더 일반화된 특성을 학습하도록 돕는다.


### 5) 최대 평행이동(가로,세로) 비율 = 0.02   
-> 멜스펙트로그램은 발화 정렬에 따라 미세한 위치 차이가 존재한다.
-> 오디오 멜스펙트럼은 정렬이 미묘하게 달라질 수 있음, 살짝 흔들어서 모델이 덜 민감하게 세팅   
-> 위 설정을 전체 학습 샘플의 50%만 적용 (원본 패턴이 사라짐 방지)   

### 6) 일부 영역 가리기   
-> 전체 학습 샘플의 25% 적용해서 과도한 왜곡 방지   
-> 이미지 전체의 2 ~ 8 % 랜덤하게 가린다.    
-> 지운 부분을 검은색으로 채움 (검은색은 즉 무음이라는 뜻 -> 음성 공백으로 처리)   

# CONVNEXT_SMALL + 이미지 전처리 결과
- F1_Score: 0.8399
- Precision: 0.8416
- Recell: 0.8407
